{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61fe8d36-5395-4533-826f-e51c5896e34a",
   "metadata": {},
   "source": [
    "# First version: Load pdf, get embeddings, query them\n",
    "\n",
    "This is based on the workflow shown here: https://medium.com/geekculture/automating-pdf-interaction-with-langchain-and-chatgpt-e723337f26a6. However, this version has been heavily altered from the version shown there. Most notably, this version uses ü§ó models instead of OpenAI models, for both text generation as well as for embeddings. This version also adds a map-reduce version, as well as a version for working with multiple papers simultaneously, with citations linking to specific pages used as sources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9a4bd4-a04f-48a4-8907-2b69e56e4321",
   "metadata": {},
   "source": [
    "### Load paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "633fc688-8266-45d2-9f3d-9866503da4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  509k  100  509k    0     0   703k      0 --:--:-- --:--:-- --:--:--  702k\n"
     ]
    }
   ],
   "source": [
    "!curl -o gptq.pdf https://arxiv.org/pdf/2210.17323.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "557d2cdf-3ffc-4a8d-950c-2110b4da2640",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader # for loading the pdf\n",
    "from langchain.embeddings import OpenAIEmbeddings # for creating embeddings\n",
    "from langchain.vectorstores import Chroma # for the vectorization part\n",
    "from langchain.chains import ChatVectorDBChain # for chatting with the pdf\n",
    "\n",
    "# Define a pprint function\n",
    "import textwrap\n",
    "def pprint(s, width=70):\n",
    "    print(textwrap.fill(s, width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69e6c57e-d0a2-4d81-9df9-234ee96969c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standard PTQ benchmarks, in the same setup as (Frantar et al., 2022). As can be seen in Table 1,\n",
      "GPTQ performs on par at 4-bit, and slightly worse than the most accurate methods at 3-bit. At the\n",
      "same time, it signiÔ¨Åcantly outperforms AdaQuant, the fastest amongst prior PTQ methods. Further,\n",
      "we compare against the full greedy OBQ method on two smaller language models: BERT-base (De-\n",
      "vlin et al., 2019) and OPT-125M. The results are shown in Appendix Table 8. At 4 bits, both methods\n",
      "perform similarly, and for 3 bits, GPTQ surprisingly performs slightly better. We suspect that this\n",
      "is because some of the additional heuristics used by OBQ, such as early outlier rounding, might\n",
      "require careful adjustments for optimal performance on non-vision models. Overall, GPTQ appears\n",
      "to be competitive with state-of-the-art post-training methods for smaller models, while taking only\n",
      "<1minute rather than ‚âà1hour. This enables scaling to much larger models.\n",
      "Runtime. Next we measure the full model quantization time (on a single NVIDIA A100 GPU) via\n",
      "GPTQ; the results are shown in Table 2. As can be seen, GPTQ quantizes 1-3 billion parameter\n",
      "models in a matter of minutes and 175B ones in a few hours. For reference, the straight-through\n",
      "based method ZeroQuant-LKD (Yao et al., 2022) reports a 3 hour runtime (on the same hardware)\n",
      "for a 1.3B model, which would linearly extrapolate to several hundred hours (a few weeks) for 175B\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "pdf_path = \"./gptq.pdf\"\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "pages = loader.load_and_split()\n",
    "print(pages[9].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218f7a65-6bac-4c8f-937a-c4a928c8134c",
   "metadata": {},
   "source": [
    "### Get embeddings of chunks of the paper\n",
    "\n",
    "We'll create a vector store, which will contain an embedding for each of the documents in our corpus.\n",
    "The reason we do this is so we can query the resulting vector store. \n",
    "Whatever topic we're interested in, we can write a (natural language) query about that topic.\n",
    "Our query can then be converted to an embedding. \n",
    "We can then find which of our documents are closest in the embedding space to our query.\n",
    "In theory, these documents should be the ones that are most relevant/similar to our query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc92aff6-9d0a-4d57-a368-fe15de31f5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a vector store of embeddings for each doc, and cluster resulting embeddings\n",
    "from utils.vector_store_tools import generate_vector_store, load_saved_vector_store\n",
    "import os\n",
    "\n",
    "username = os.environ.get('USER')\n",
    "cache_loc = os.path.join('/','scratch',username,'hf_cache')\n",
    "\n",
    "saved_already = True\n",
    "\n",
    "if saved_already:\n",
    "    db = load_saved_vector_store(cache_loc)\n",
    "else:\n",
    "    db = generate_vector_store(pages, cache_loc)\n",
    "    db.save_local(os.path.join(\"data\",\"faiss_index\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a95290a-1fc9-479f-abb4-37a7cac08962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 DOCUMENTS RETURNED.\n",
      "\n",
      "FIRST DOCUMENT CONTENT: \n",
      "Figure 1: Different finetuning methods and their memory requirements. QLORAimproves over LoRA by\n",
      "quantizing the transformer model to 4-bit precision and using paged optimizers to handle memory spikes.\n",
      "2 Background\n",
      "Block-wise k-bit Quantization Quantization is the process of discretizing an input from a rep-\n",
      "resentation that holds more information to a representation with less information. It often means\n",
      "taking a data type with more bits and converting it to fewer bits, for example from 32-bit floats to\n",
      "8-bit Integers. To ensure that the entire range of the low-bit data type is used, the input data type is\n",
      "commonly rescaled into the target data type range through normalization by the absolute maximum\n",
      "of the input elements, which are usually structured as a tensor. For example, quantizing a 32-bit\n",
      "Floating Point (FP32) tensor into a Int8 tensor with range [‚àí127,127]:\n",
      "XInt8=round\u0012127\n",
      "absmax (XFP32)XFP32\u0013\n",
      "=round (cFP32¬∑XFP32), (1)\n",
      "where cis the quantization constant orquantization scale . Dequantization is the inverse:\n",
      "dequant (cFP32,XInt8) =XInt8\n",
      "cFP32=XFP32(2)\n",
      "The problem with this approach is that if a large magnitude value (i.e., an outlier) occurs in the input\n",
      "tensor, then the quantization bins‚Äîcertain bit combinations‚Äîare not utilized well with few or no\n",
      "numbers quantized in some bins. To prevent the outlier issue, a common approach is to chunk the\n",
      "input tensor into blocks that are independently quantized, each with their own quantization constant c.\n",
      "This can be formalized as follows: We chunk the input tensor X‚ààRb√óhintoncontiguous blocks of\n",
      "sizeBby flattening the input tensor and slicing the linear segment into n= (b√óh)/Bblocks. We\n",
      "quantize these blocks independently with Equation 1 to create a quantized tensor and nquantization\n",
      "constants ci.\n",
      "Low-rank Adapters Low-rank Adapter (LoRA) finetuning [ 28] is a method that reduces memory\n",
      "requirements by using a small set of trainable parameters, often termed adapters, while not updating\n",
      "the full model parameters which remain fixed. Gradients during stochastic gradient descent are\n",
      "passed through the fixed pretrained model weights to the adapter, which is updated to optimize the\n",
      "loss function. LoRA augments a linear projection through an additional factorized projection. Given\n",
      "a projection XW =YwithX‚ààRb√óh,W‚ààRh√óoLoRA computes:\n",
      "Y=XW +sXL 1L2, (3)\n",
      "whereL1‚ààRh√órandL2‚ààRr√óo, and sis a scalar.\n",
      "Memory Requirement of Parameter-Efficient Finetuning One important point of discussion is\n",
      "the memory requirement of LoRA during training both in terms of the number and size of adapters\n",
      "used. Since the memory footprint of LoRA is so minimal, we can use more adapters to improve\n",
      "performance without significantly increasing the total memory used. While LoRA was designed as a\n",
      "3\n",
      "\n",
      "SOURCE: \n",
      "qlora.pdf, page 2\n"
     ]
    }
   ],
   "source": [
    "# Test out the db\n",
    "query_docs = db.similarity_search(query = 'What is quantization of model parameters?')\n",
    "\n",
    "print(f'{len(query_docs)} DOCUMENTS RETURNED.\\n')\n",
    "print(f'FIRST DOCUMENT CONTENT: \\n{query_docs[0].page_content}')\n",
    "print(f\"\\nSOURCE: \\n{query_docs[0].metadata['source']}, page {query_docs[0].metadata['page']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1736ea70-8187-478b-802b-d6afda39b822",
   "metadata": {},
   "source": [
    "### Load an LLM that will answer questions about the paper\n",
    "\n",
    "We need a smart AI chatbot to answer our questions about the paper using the documents in our vector store.\n",
    "Let's use LLaMA-2 7B -- it's pretty fast on this hardware.\n",
    "If you have money and aren't too concerned about privacy, you could use GPT-3 or GPT-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2206a0c6-7e59-4170-8051-df1d8b7e7b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, using /scratch/cehrett/hf_cache for huggingface cache. Models will be stored there.\n",
      "Huggingface API key loaded.\n",
      "Loading model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63071afae05a457180bdd66018710077",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer\n",
      "Instantiating pipeline\n",
      "Instantiating HuggingFacePipeline\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "from utils import load_model\n",
    "llm, _, cache_loc = load_model.load_model(model_id=\"meta-llama/Llama-2-7b-chat-hf\", max_length=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc66cba9-939d-454e-ba22-23412ae38996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SilenceBot: SILENCE!!!\n"
     ]
    }
   ],
   "source": [
    "# Let's test out the LLM\n",
    "test_output = llm(prompt=\"\"\"\\\n",
    "You are a SilenceBot. Whatever the user says to you, you respond only with: \"SILENCE!!!!\" \n",
    "\n",
    "User: Hi SilenceBot, can I talk to you for a minute?\n",
    "\"\"\")\n",
    "print(test_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dabb8c-7b35-47fb-b9a3-bc2d0e366c05",
   "metadata": {},
   "source": [
    "### Define an object that will query the embeddings to get context for a question to the LLM\n",
    "The `ChatVectorDBChain` object is a wrapper around our LLM, that provides a way of interacting with that LLM.\n",
    "When we use this chain to interact with our LLM, the LLM will see a prompt containing our question. \n",
    "But more than just our question, the prompt will also contain passages from the paper we loaded.\n",
    "Which passages will the LLM get to see? The ones that are most relevant to our question, as measured by cosine similarity of the embeddings.\n",
    "\n",
    "So, when we pass a question to the chain:\n",
    "* The question is converted to an embedding\n",
    "* The embedding of our question is used to find the most similar documents in our vector store\n",
    "* The question is then shown to our LLM, along with the most similar documents, which the LLM uses as context to answer our question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d59313c6-531d-4aaf-9d11-0a6596d2b8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      " When the authors write \"quantization\", they are referring\n",
      "specifically to the process of representing model parameters using\n",
      "fewer bits than the standard floating point representation. They use\n",
      "the phrase \"parameter quantization\" to distinguish this meaning from\n",
      "other possible interpretations... Of course, there are many other ways\n",
      "to \"quantify\" a neural network (e.g., quantifying gradients used\n",
      "during training, etc.), but if the text is talking about reducing the\n",
      "precision of model weights themselves, then this is what they mean.\n",
      "Source pages:\n",
      "2,2,1,3\n"
     ]
    }
   ],
   "source": [
    "pdf_qa = ChatVectorDBChain.from_llm(llm,\n",
    "                                    db, \n",
    "                                    return_source_documents=True)\n",
    "\n",
    "\n",
    "query = 'What is \"quantization of model parameters\"? What does that mean?'\n",
    "result = pdf_qa({\"question\": query, \"chat_history\": \"\"})\n",
    "\n",
    "\n",
    "print(\"Answer:\")\n",
    "pprint(result[\"answer\"])\n",
    "print(\"Source pages:\\n\"+\",\".join([str(page.metadata['page']) for page in result['source_documents']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298eb736-7118-4a05-ac09-102a0c25e261",
   "metadata": {},
   "source": [
    "## Now make a map-reduce version\n",
    "\n",
    "The above version is simple and fast, but it has limitations. \n",
    "* It doesn't use the specific prompt format LLaMA-2 expects.\n",
    "* For context, the model just gets big chunks of the paper, pasted together. Lots of that context is probably irrelevant or distracting.\n",
    "\n",
    "So, instead of a one-step Q&A like used above, let's define a map-reduce approach that is more nuanced. In the map step, the model looks at just one document in the corpus at a time, and outputs a response about just that document.\n",
    "The map step gets applied to lots of documents (one at a time).\n",
    "In the reduce step, the model looks at all the outputs it produced during the map stage, and uses all those outputs combined as context to produce a final output.\n",
    "The prompt used for the map step is different from the one used for the reduce step.\n",
    "\n",
    "So, when we pass a question to this chain:\n",
    "* The question is converted to an embedding\n",
    "* The embedding of our question is used to find the most similar documents in our vector store\n",
    "* For each of those most similar documents:\n",
    "    * The document is shown to the LLM, along with our question. The model is instructed to summarize all info in the document relevant to our question.\n",
    "* There is now a summary of each of the documents similar to our query.\n",
    "* Those summaries are pasted together and shown to the LLM along with our question.\n",
    "* The LLM produces a final answer to our question using the summarized documents as context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c2589dbc-64d7-4ccf-8fca-c653a65cfb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define map and reduce functions\n",
    "from tqdm import tqdm\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "def map_fn(doc, \n",
    "           query,\n",
    "           verbose=False\n",
    "          ):\n",
    "    \n",
    "    prompt = \\\n",
    "f\"\"\"\\\n",
    "[INST] <<SYS>>\n",
    "From the following part of an academic paper, summarize all information that is relevant to the question \"{query}\\\".\n",
    "<</SYS>>\n",
    "\n",
    "DOCUMENT:\n",
    "{doc.page_content} [/INST]\\\n",
    "\"\"\"\n",
    "    \n",
    "    output = llm(prompt)\n",
    "    if verbose:\n",
    "        print(f'DOCUMENT:\\n{doc.page_content}\\n\\nSUMMARY:\\n{output}')\n",
    "    return output\n",
    "\n",
    "def reduce_fn(mapped_outputs, query):\n",
    "    context = \"\\n#########\\n\".join(mapped_outputs)\n",
    "    \n",
    "    prompt = \\\n",
    "f\"\"\"\\\n",
    "[INST] <<SYS>>\n",
    "Based on the below context, which is summaries of parts of academic papers, answer the question: \\\"{query}\\\".\n",
    "<</SYS>>\n",
    "\n",
    "CONTEXT:\n",
    "{context} [/INST]\\\n",
    "\"\"\"\n",
    "    \n",
    "    return llm(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1ac1898b-4991-41a9-87be-c62b8d284814",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping documents: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [03:22<00:00, 40.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL ANSWER:\n",
      "  Quantization of model parameters refers to the process of representing a set of model parameters using fewer bits than their original size. This technique is commonly used to reduce the memory requirement for training deep neural networks, especially when dealing with large models like transformers. The most common way to perform quantization is by dividing the weight tensor into chunks, assigning a unique quantization constant to each chunk, and then scaling the standard deviations of the weight tensor to match the standard deviations of the k-bit data type. Another approach is to allow individual weights to move freely during training, so that they can adaptively adjust their discrete values in response to changing error gradients. Techniques like optimal brain quantization (OBQ) and GPTQ have been proposed recently to improve the efficiency of training large language models.\n",
      "\n",
      "Source pages:\n",
      "2,2,4,2,3\n"
     ]
    }
   ],
   "source": [
    "# Try out the map-reduce version\n",
    "query = \"What is quantization of model parameters?\"\n",
    "\n",
    "from utils.vector_store_tools import get_relevant_docs\n",
    "docs = get_relevant_docs(query, db)\n",
    "\n",
    "# Use tqdm for list comprehension progress bar\n",
    "verbose = False\n",
    "mapped_outputs = [map_fn(doc, query, verbose=verbose) for doc in tqdm(docs, desc=\"Mapping documents\")]\n",
    "\n",
    "final_answer = reduce_fn(mapped_outputs, query)\n",
    "\n",
    "print(\"FINAL ANSWER:\")\n",
    "print(final_answer)\n",
    "print(\"\\nSource pages:\\n\" + \",\".join([str(page.metadata['page']) for page in docs]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a2a832-83f5-4cd0-b594-7c1fd634ef52",
   "metadata": {},
   "source": [
    "## Now make a version that loads multiple papers and cites them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bbb77150-17bb-409f-b13a-72ca54108e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  509k  100  509k    0     0   676k      0 --:--:-- --:--:-- --:--:--  676k\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1040k  100 1040k    0     0  1469k      0 --:--:-- --:--:-- --:--:-- 1469k\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1571k  100 1571k    0     0  2063k      0 --:--:-- --:--:-- --:--:-- 2062k\n"
     ]
    }
   ],
   "source": [
    "!curl -o gptq.pdf https://arxiv.org/pdf/2210.17323.pdf\n",
    "!curl -o qlora.pdf https://arxiv.org/pdf/2305.14314.pdf\n",
    "!curl -o lora.pdf https://arxiv.org/pdf/2106.09685.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f85f8069-5ab5-4b22-a728-156d3dedc159",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_paths = [\"gptq.pdf\", \"qlora.pdf\", \"lora.pdf\"]\n",
    "pages = []\n",
    "for pdf_path in pdf_paths:\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    pages += loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cf0ba511-1af4-47f6-86d9-3495b7b1ab6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_already = True\n",
    "\n",
    "if saved_already:\n",
    "    db = load_saved_vector_store(cache_loc)\n",
    "else:\n",
    "    db = generate_vector_store(pages, cache_loc)\n",
    "    db.save_local(os.path.join(\"data\",\"faiss_index\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "af3d95f7-bdf1-4149-a9e9-d44de5250769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "Quantization can refer to the procedure of mapping continuous signals\n",
      "or numbers to shorter strings of binary digits for implementation on\n",
      "computers. In this setting, continuous quantities represented by\n",
      "floating-point numbers may lose some of their resolution when\n",
      "converted to integers due to roundoff mistakes. By choosing an\n",
      "appropriate nonzero value called a \"quantization reference\" or simply\n",
      "\"quant,\" some fraction of the integers might be assigned the same code\n",
      "word as the ones closest to the origanal quantity within a given\n",
      "measurement interval (or boxcar); otherwise, there would be\n",
      "\"aliasing.\" Aliasing could occur with quantization unless properly\n",
      "controlled via the selection of appropriate sample intervals and\n",
      "oversampling rates depending upon desired accuracy levels attainable\n",
      "within available computing hardware capabilities. It also finds\n",
      "applications across various disciplines such as signal processing,\n",
      "embedded systems design, robotics kinematics, image processing,\n",
      "telecommunications networking protocol design, software engineering\n",
      "productivity metrics analysis tool development frameworks artificial\n",
      "intelligence reinforcement learning agents behavioral economics models\n",
      "applied microbiology pharmaceutical drug efficacy testing clinical\n",
      "trials research studies neuroscience cognitive psychology educational\n",
      "assessments social sciences survey methods anthropometrics healthcare\n",
      "delivery medical informatics hospital management system designs\n",
      "patient flow tracking resource allocation algorithms electronic health\n",
      "records security access control user authentication usability\n",
      "interface visual presentation web browser applications operating\n",
      "systems mobile device platforms computer vision object recognition\n",
      "natural language processing sentiment analysis expert systems user\n",
      "interfaces intelligent systems user experience UX evaluation remote\n",
      "sensing geospatial analysis satellite imagery digital elevation models\n",
      "topographic maps distance education multimedia courseware online\n",
      "tutoring chatbot conversational UI human machine interaction HMI voice\n",
      "activated IVR speech recognition speech synthesis text-to-speech TTS\n",
      "machine translation MT dialogue systems DSS decision support systems\n",
      "eCommerce recommendation engines predictive analytics credit risk\n",
      "scoring loan approval financial portfolio optimization wealth\n",
      "management retirement planning estate tax planning charitable giving\n",
      "legal services document management workflow automation project\n",
      "management construction site safety monitoring building inspection\n",
      "scheduling facility maintenance equipment inventory management supply\n",
      "chain logistics transportation management energy efficiency climate\n",
      "change mitigation sustainable development Go green! Quantization comes\n",
      "under three categories i) Discrete quantization ii) Continuous\n",
      "quantization & iii) Vector quantization. Discrete quantization\n",
      "involves taking a real- valued number xand representing its integer\n",
      "multiples xmfor certain mwhere mcan take any valid integer value\n",
      "greater than or equal to unity according to the chosen quantization\n",
      "step size Œµthat separates adjacent bin centers Cm. When quantizing to\n",
      "q bits, then each value must fall exactly within one possible bucket ‚Äî\n",
      "otherwise overflow occurs due to wrapping around after reaching either\n",
      "endpoint (maximum or minimum). The choice of quantization step\n",
      "sizesensitively affects accuracy, tradeoffs being made between\n",
      "reducing overall precisions at cost of larger steps leading potential\n",
      "errors while simultaneously improving interpretability through\n",
      "coarsening representations close enough together. Moreover, it can\n",
      "also happen in practice that neighboring samples get mapped near\n",
      "neighbors rather than identical ones - this phenomenon known as\n",
      "aliasing must carefully managed whenever dealing sensitive data\n",
      "subjects throughout application domain wherever appropriate safeguards\n",
      "should put forth effort protect privacy confidentiality against\n",
      "unauthorized attempts hack breach compromise leak expose any form\n",
      "encryption security lapse exposure vulnerabilities weakness inherent\n",
      "susceptibility design flaw oversight laxity accountability regulatory\n",
      "compliance neglect enforcement lack interest involvement ownership\n",
      "stewardship responsible governance ethical considerations principles\n",
      "standards regulations best practices guidelines rule law precedents\n",
      "judicial decisions jurisprudence.\n",
      "Sources:\n",
      "gptq.pdf, page2\n",
      "qlora.pdf, page2\n",
      "qlora.pdf, page4\n",
      "qlora.pdf, page3\n"
     ]
    }
   ],
   "source": [
    "# Simple version\n",
    "pdf_qa = ChatVectorDBChain.from_llm(llm,\n",
    "                                    db, \n",
    "                                    return_source_documents=True)\n",
    "\n",
    "\n",
    "result = pdf_qa({\"question\": query, \"chat_history\": \"\"})\n",
    "print(\"Answer:\")\n",
    "pprint(result[\"answer\"])\n",
    "print(\"Sources:\\n\"+\"\\n\".join([page.metadata['source'] + \\\n",
    "                              ', page' + str(page.metadata['page']) \\\n",
    "                              for page in result['source_documents']]))\n",
    "\n",
    "query = 'Please explain what \"quantization of model parameters\" is. What do they mean by that?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86a95e2-e356-4737-94a5-6e6a4c85ec7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMS Environment",
   "language": "python",
   "name": "llms_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
